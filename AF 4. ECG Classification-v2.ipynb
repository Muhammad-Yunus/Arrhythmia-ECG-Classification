{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yunus\\Anaconda3\\envs\\GPU_ENV\\lib\\site-packages\\ipykernel\\parentpoller.py:116: UserWarning: Parent poll failed.  If the frontend dies,\n",
      "                the kernel may be left running.  Please let us know\n",
      "                about your system (bitness, Python, etc.) at\n",
      "                ipython-dev@scipy.org\n",
      "  ipython-dev@scipy.org\"\"\")\n"
     ]
    }
   ],
   "source": [
    "labels = ['AF', 'N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# disable GPU\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "dataset_folder = 'dataset/'\n",
    "filenames = []\n",
    "for filename in os.listdir(dataset_folder):\n",
    "    if filename.find(\"_all\") > -1 :\n",
    "        filenames.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_all.csv',\n",
       " 'test_all_lms.csv',\n",
       " 'test_all_nlms.csv',\n",
       " 'test_all_noised.csv',\n",
       " 'test_all_noised_-5db.csv',\n",
       " 'test_all_rls.csv',\n",
       " 'train_all.csv',\n",
       " 'train_all_lms.csv',\n",
       " 'train_all_nlms.csv',\n",
       " 'train_all_noised.csv',\n",
       " 'train_all_noised_-5db.csv',\n",
       " 'train_all_rls.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_lms_df = pd.read_csv(dataset_folder + \"train_all_lms.csv\", header=None)\n",
    "# train_nlms_df = pd.read_csv(dataset_folder + \"train_all_nlms.csv\", header=None)\n",
    "# train_rls_df = pd.read_csv(dataset_folder + \"train_all_rls.csv\", header=None)\n",
    "# train_ori_df = pd.read_csv(dataset_folder + \"train_all.csv\", header=None)\n",
    "train_noised_df = pd.read_csv(dataset_folder + \"train_all_noised_-5db.csv\", header=None)\n",
    "\n",
    "# test_lms_df = pd.read_csv(dataset_folder + \"test_all_lms.csv\", header=None)\n",
    "# test_nlms_df = pd.read_csv(dataset_folder + \"test_all_nlms.csv\", header=None)\n",
    "# test_rls_df = pd.read_csv(dataset_folder + \"test_all_rls.csv\", header=None)\n",
    "# test_ori_df = pd.read_csv(dataset_folder + \"test_all.csv\", header=None)\n",
    "test_noised_df = pd.read_csv(dataset_folder + \"test_all_noised_-5db.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoised = 'noised' # isi dengan 'lms', 'nlms', 'rls' untuk memilih sumber dataset dari hasil denoising tsb.\n",
    "                  # isi dengan 'ori' jika ingin menggunakan original dataset\n",
    "                  # isi dengan 'noised' jika ingin menggunakan noised dataset\n",
    "\n",
    "if denoised == 'lms':\n",
    "    train_df = train_lms_df\n",
    "    test_df = test_lms_df\n",
    "elif denoised == 'nlms':\n",
    "    train_df = train_nlms_df\n",
    "    test_df = test_nlms_df\n",
    "elif denoised == 'rls':\n",
    "    train_df = train_rls_df\n",
    "    test_df = test_rls_df\n",
    "elif denoised == 'ori':\n",
    "    train_df = train_ori_df\n",
    "    test_df = test_ori_df\n",
    "elif denoised == 'noised':\n",
    "    train_df = train_noised_df\n",
    "    test_df = test_noised_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset AFDB\n",
    "Dataset AFDB terdiri dari 300 kolom, dimana kolom ke 300 merupakan katerori/class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_lms_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-6e0a29491dbe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_lms_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_lms_df' is not defined"
     ]
    }
   ],
   "source": [
    "train_lms_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lms_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lms_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_df = pd.concat([train_df, test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    22629\n",
      "0    18657\n",
      "Name: 300, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "ecg_df[300]=ecg_df[300].astype(int)\n",
    "equilibre=ecg_df[300].value_counts()\n",
    "\n",
    "print(equilibre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling and resampling dataset\n",
    "\n",
    "from sklearn.utils import resample\n",
    "n_samples = 30000 \n",
    "random_states = [123, 124]\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for i in range(len(equilibre)):\n",
    "    dfs.append(ecg_df[ecg_df[300]==i])\n",
    "    dfs[i]=resample(dfs[i],replace=True,n_samples=n_samples,random_state=random_states[i])\n",
    "\n",
    "ecg_df=pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    30000\n",
      "0    30000\n",
      "Name: 300, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "ecg_df[300]=ecg_df[300].astype(int)\n",
    "equilibre=ecg_df[300].value_counts()\n",
    "\n",
    "print(equilibre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train = ecg_df[300]\n",
    "y = to_categorical(target_train)\n",
    "\n",
    "X = ecg_df.iloc[:,:300].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                    X, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((51000, 300), (9000, 300), (51000, 2), (9000, 2))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Convolutional Neural Network\n",
    "\n",
    "- Import Keras library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, MaxPool1D, Flatten, Dropout\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Buat CNN Model dengan aritektur network : \n",
    "`CONV-POOL-CONV-POOL-CONV-POOL-FC`\n",
    "- CONV : 1D Convolutional Layer\n",
    "- POOL : MAX Pooling Layer\n",
    "- FC   : Dense Layer + Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(max_len):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv1D(filters=64,\n",
    "                     kernel_size=5,\n",
    "                     activation='relu',\n",
    "                     input_shape=(max_len, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool1D(pool_size=2,\n",
    "                        strides=2,\n",
    "                        padding='same'))\n",
    "    \n",
    "    \n",
    "    model.add(Conv1D(filters=64,\n",
    "                     kernel_size=3,\n",
    "                     activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool1D(pool_size=2,\n",
    "                        strides=2,\n",
    "                        padding='same'))\n",
    "    \n",
    "    \n",
    "    model.add(Conv1D(filters=64,\n",
    "                     kernel_size=3,\n",
    "                     activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool1D(pool_size=2,\n",
    "                        strides=2,\n",
    "                        padding='same'))\n",
    "    \n",
    "    \n",
    "    # Fully Connected layer (FC)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, \n",
    "                    activation='relu'))\n",
    "    model.add(Dense(32, \n",
    "                    activation='relu'))\n",
    "    model.add(Dense(2, \n",
    "                    activation='softmax'))\n",
    "              \n",
    "    model.summary()\n",
    "    model.compile(optimizer='adam', \n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sekarang kita akan melakukan proses training model dengan memanfaatkan `.fit()` pada model yang kita buat diatas.\n",
    "- selain itu kita gunakan juka teknik `EarlyStoping()` untuk menghentikan proses training jika terjadi divergensi pada validation data yang diakibatkan oleh overfitting. \n",
    "- pada `EarlyStoping()` kita gunakan parmeter `patience=8` yang artinya jika proses training untuk 8 epoch tidak terjadi peningkatan maka hentikan proses training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model(model_, x, y, x_val, y_val, epochs_, batch_size_):\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=8),\n",
    "                 ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "    hist = model_.fit(x, \n",
    "                      y,\n",
    "                      epochs=epochs_,\n",
    "                      callbacks=callbacks, \n",
    "                      batch_size=batch_size_,\n",
    "                      shuffle=True,\n",
    "                      validation_data=(x_val,y_val))\n",
    "    model_.load_weights('best_model.h5')\n",
    "    return hist "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prepare training set dan test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(len(X_train), X_train.shape[1],1)\n",
    "X_test = X_test.reshape(len(X_test), X_test.shape[1],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- jalankan proses training dengan `EPOCH` sebanyak 16 dan `BATCH_SIZE` sebesar 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 296, 64)           384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 296, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 148, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 146, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 146, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 73, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 71, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 71, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 36, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               295040    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 325,090\n",
      "Trainable params: 324,706\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "Train on 51000 samples, validate on 9000 samples\n",
      "Epoch 1/16\n",
      "51000/51000 [==============================] - 65s 1ms/step - loss: 0.5756 - acc: 0.6969 - val_loss: 0.5226 - val_acc: 0.7388\n",
      "Epoch 2/16\n",
      "51000/51000 [==============================] - 56s 1ms/step - loss: 0.4959 - acc: 0.7583 - val_loss: 0.4680 - val_acc: 0.7720\n",
      "Epoch 3/16\n",
      "51000/51000 [==============================] - 55s 1ms/step - loss: 0.4159 - acc: 0.8096 - val_loss: 0.4218 - val_acc: 0.8127\n",
      "Epoch 4/16\n",
      "51000/51000 [==============================] - 56s 1ms/step - loss: 0.3315 - acc: 0.8575 - val_loss: 0.3629 - val_acc: 0.8453\n",
      "Epoch 5/16\n",
      "51000/51000 [==============================] - 56s 1ms/step - loss: 0.2654 - acc: 0.8903 - val_loss: 0.3468 - val_acc: 0.8599\n",
      "Epoch 6/16\n",
      "51000/51000 [==============================] - 58s 1ms/step - loss: 0.2078 - acc: 0.9173 - val_loss: 0.3266 - val_acc: 0.8813\n",
      "Epoch 7/16\n",
      "51000/51000 [==============================] - 62s 1ms/step - loss: 0.1697 - acc: 0.9339 - val_loss: 0.3474 - val_acc: 0.8826\n",
      "Epoch 8/16\n",
      "51000/51000 [==============================] - 60s 1ms/step - loss: 0.1475 - acc: 0.9439 - val_loss: 0.3256 - val_acc: 0.8908\n",
      "Epoch 9/16\n",
      "51000/51000 [==============================] - 59s 1ms/step - loss: 0.1265 - acc: 0.9524 - val_loss: 0.3349 - val_acc: 0.8957\n",
      "Epoch 10/16\n",
      "51000/51000 [==============================] - 59s 1ms/step - loss: 0.1132 - acc: 0.9559 - val_loss: 0.3492 - val_acc: 0.8966\n",
      "Epoch 11/16\n",
      "51000/51000 [==============================] - 59s 1ms/step - loss: 0.1058 - acc: 0.9597 - val_loss: 0.3467 - val_acc: 0.9013\n",
      "Epoch 12/16\n",
      "51000/51000 [==============================] - 59s 1ms/step - loss: 0.0920 - acc: 0.9659 - val_loss: 0.3717 - val_acc: 0.9026\n",
      "Epoch 13/16\n",
      "51000/51000 [==============================] - 60s 1ms/step - loss: 0.0880 - acc: 0.9675 - val_loss: 0.3462 - val_acc: 0.9043\n",
      "Epoch 14/16\n",
      "51000/51000 [==============================] - 59s 1ms/step - loss: 0.0786 - acc: 0.9708 - val_loss: 0.3656 - val_acc: 0.9046\n",
      "Epoch 15/16\n",
      "39808/51000 [======================>.......] - ETA: 12s - loss: 0.0796 - acc: 0.9698"
     ]
    }
   ],
   "source": [
    "max_len = X_train.shape[1]  \n",
    "\n",
    "EPOCHS = 16\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "model = cnn_model(max_len)\n",
    "history=check_model(model, X_train,y_train,X_test,y_test, EPOCHS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"CNN_Classification_model_%s.h5\" % denoised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model\n",
    "\n",
    "- Plot Accuracy vs Epochs\n",
    "- Plot Loss vs Epochs\n",
    "- Plot Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(history, X_test, y_test):\n",
    "    \n",
    "    fig1, ax_acc = plt.subplots()\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Model - Accuracy')\n",
    "    plt.legend(['Training', 'Validation'], loc='lower right')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    fig2, ax_loss = plt.subplots()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Model - Loss')\n",
    "    plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "evaluate_model(history, X_test, y_test)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dari hasil diatas kita bisa melihat jika akurasi training set = 0.99, sedangkan untuk akurasi validation set = 0.97\n",
    "- Untuk plot loss training set = 0.01 sedangkan untuk loss validation set = 0.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test data\n",
    "y_pred=model.predict(X_test)\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(cnf_matrix, classes=['AF', 'N'],normalize=True,\n",
    "                      title='Confusion matrix, with normalization')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dari hasil plot confusion matrix, dapat dilihat tiap kelas memiliki banyak TRUE POSITIVE predicted data\n",
    "- semakin gelap kebiruan menunjukan banyaknya hasil predicted label untuk true label tersebut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test.argmax(axis=1), \n",
    "                            y_pred.argmax(axis=1), \n",
    "                            target_names=['AF', 'N']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Jika kita lihat, nilai report untuk seluruh klas juga bagus, \n",
    "- Nilai recall dan precission juga tinggi, menunjukan model mampu memprediksi data dengan baik untuk seluruh data pada class tersebut "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:GPU_ENV]",
   "language": "python",
   "name": "conda-env-GPU_ENV-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
